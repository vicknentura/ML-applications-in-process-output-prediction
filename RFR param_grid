#classification model
param_grid2 = {
    'n_estimators': [10, 50, 100, 200],  # Number of trees in the forest
    'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],  # Function to measure the quality of a split
    'max_depth': [None, 10, 20, 30, 40, 50],  # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node
    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],  # Minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node
    'max_features': ['sqrt', 'log2', None],  # The number of features to consider when looking for the best split
    'max_leaf_nodes': [None, 10, 20, 30],  # Grow trees with a maximum number of leaf nodes
    'min_impurity_decrease': [0.0, 0.1],  # A node will be split if this split induces a decrease of the impurity greater than or equal to this value
    'bootstrap': [True, False],  # Whether bootstrap samples are used when building trees
    'oob_score': [True, False],  # Whether to use out-of-bag samples to estimate the generalization accuracy
    'n_jobs': [-1],  # The number of jobs to run in parallel (-1 means using all processors)
    'random_state': [42],  # For reproducibility
    'verbose': [0, 1],  # Controls the verbosity when fitting and predicting
    'warm_start': [True, False],  # When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble
    'ccp_alpha': [0.0, 0.1, 0.2],  # Complexity parameter used for Minimal Cost-Complexity Pruning
    'max_samples': [None, 0.5, 0.8],  # If bootstrap is True, the number of samples to draw from X to train each base estimator
}
